pretrained_model: null # pretrained QAModel model from list_available_models()
trainer:
  gpus: 1 # the number of gpus, 0 for CPU, or list with gpu indices
  num_nodes: 1
  max_epochs: 5 # the number of training epochs
  max_steps: null # precedence over max_epochs
  accumulate_grad_batches: 2 # accumulates grads every k batches
  precision: 16 # 16 to use AMP
  amp_level: O1 # O1 or O2 if using AMP
  accelerator: ddp
  gradient_clip_val: 1.0
  val_check_interval: 1.0 # check once per epoch .25 for 4 times per epoch
  checkpoint_callback: false # provided by exp_manager
  logger: false # provided by exp_manager
  num_sanity_val_steps: 0
  log_every_n_steps: 2000  # Interval of logging.
  # replace_sampler_ddp: false

exp_manager:
  exp_dir: null # where to store logs and checkpoints
  name: null # name of experiment
  explicit_log_dir: ./2021gatortron/nemo_downstream/qa/results/emrqa_med02_9b_50k
  create_tensorboard_logger: False
  create_checkpoint_callback: True

model:
  nemo_path: ${exp_manager.explicit_log_dir}/default.nemo # exported .nemo path
  dataset:
    #./2021gatortron/data/qa/n2c2-community-annotations_2014-pampari-question-answering/processed_data/emrqa
    data_dir: ./2021gatortron/data/qa/n2c2-community-annotations_2014-pampari-question-answering/processed_data/new_med_rel
    # medication-train-sampled-0.5.json relation-train.json relation-train-sampled-0.3.json
    train_fn: medication-train-sampled-0.2.json
    dev_fn: medication-test.json
    test_fn: medication-test.json
    version_2_with_negative: false
    # If true, the examples contain some that do not have an answer.
    doc_stride: 384
    # When splitting up a long document into chunks,
    # how much stride to take between chunks.
    max_query_length: 96
    # The maximum number of tokens for the question.
    # Questions longer than this will be truncated to
    # this length.
    max_seq_length: 512
    # The maximum total input sequence length after
    # WordPiece tokenization. Sequences longer than this
    # will be truncated, and sequences shorter than this
    # will be padded.
    max_answer_length: 96
    # The maximum length of an answer that can be
    # generated. This is needed because the start
    # and end predictions are not conditioned
    # on one another.
    null_score_diff_threshold: 0.0
    # If null_score - best_non_null is greater than the threshold predict null.
    n_best_size: 5
    # The total number of n-best predictions to generate at testing.
    use_cache: true
    do_lower_case: false

    num_workers:  2
    pin_memory: true
    drop_last: false

  train_ds:
    file: ${model.dataset.data_dir}/${model.dataset.train_fn} # .json file
    batch_size: 4 # per GPU
    shuffle: true
    num_samples: -1
    use_cache: ${model.dataset.use_cache}
    # Default values for the following params are retrieved from dataset config section, but you may override them
    num_workers: ${model.dataset.num_workers}
    drop_last: ${model.dataset.drop_last}
    pin_memory: ${model.dataset.pin_memory}

  validation_ds:
    file: ${model.dataset.data_dir}/${model.dataset.dev_fn} # .json file
    batch_size: 4 # per GPU
    shuffle: false
    num_samples: -1  
    use_cache: ${model.dataset.use_cache}  
    # Default values for the following params are retrieved from dataset config section, but you may override them
    num_workers: ${model.dataset.num_workers}
    drop_last: ${model.dataset.drop_last}
    pin_memory: ${model.dataset.pin_memory}

  test_ds:
    file: ${model.dataset.data_dir}/${model.dataset.test_fn} # .json file
    batch_size: 4 # per GPU
    shuffle: false
    num_samples: -1
    use_cache: ${model.dataset.use_cache}
    # Default values for the following params are retrieved from dataset config section, but you may override them
    num_workers: ${model.dataset.num_workers}
    drop_last: ${model.dataset.drop_last}
    pin_memory: ${model.dataset.pin_memory}
    
  tokenizer:
    tokenizer_name: megatron-bert-cased # tokenizer that inherits from TokenizerSpec
    vocab_file: ./new_vocabs/uf_full_pubmed_wiki_cased_50k/vocab.txt # path to vocab file
    tokenizer_model: null # only used if tokenizer is sentencepiece
    special_tokens: null # expand the following to a dictionary if special tokens need to be added.
    #  only necessary for adding transformer/bert-specific special tokens to tokenizer if the tokenizer does not already have these inherently.

  language_model:
    pretrained_model_name: megatron-bert-cased
    lm_checkpoint: ./models/new_pretraining_checkpoint/9b_uf_pubmed_wiki_uf_pubmed_wiki_50k_cased/iter_0670000 #iter_0900000 iter_0950000
    config_file: ./models/new_pretraining_checkpoint/9b_uf_pubmed_wiki_uf_pubmed_wiki_50k_cased/config.json
    config: null # if specified initializes model from scratch

  token_classifier:
    num_layers: 1
    dropout: 0.0
    num_classes: 2
    activation: relu
    log_softmax: false
    use_transformer_init: true


  optim:
    name: adamw
    lr: 1e-5
    weight_decay: 0.0
    sched:
      name: SquareRootAnnealing

      # pytorch lightning args
      monitor: val_loss
      reduce_on_plateau: false

      # scheduler config override
      warmup_steps: null
      warmup_ratio: 0.0
      last_epoch: -1

hydra:
  run:
    dir: .
  job_logging:
    root:
      handlers: null